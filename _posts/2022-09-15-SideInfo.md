---
title: "Learning with Side Information"
layout: post
---

+ a
+ b
* TOC
{:toc}

> ### The Key Points
>  We introduce our first  multi-variate feature extraction problem, the problem of learning with side information. Here, we wish to select a $k$-dimensional feature of the data $\mathsf x$ that carries useful information to infer the value of the label $\mathsf y$, while excluding the information that a jointly distributed side information $\mathsf s$ can provide. We use this example to develop the underlying geometric structure of multi-variate dependence, and demonstrate how to use the nested H-Score networks to make projections according to these structures to get good solutions. 

## Previously
We started by observing that the dependence between two random variables $\mathsf x$ and $\mathsf y$ can be decomposed into a number of modes, each mode as a simple correlation between a feature $f_i(\mathsf x)$ and a feature $g_i(\mathsf y)$. We formulated the [modal decomposition](https://gilearning.github.io/ModalDecomposition/) as the optimization problem

$$
\begin{equation}
 (f^\ast_{[k]}, g^\ast_{[k]}) =\arg\min_{(f_1, \ldots f_k \in \mathcal {F_X}, g_1, \ldots g_k \in \mathcal {F_Y})} \; \left\Vert \mathrm{PMI}- \left(\sum_{i=1}^k  f_i \otimes g_i\right)\right\Vert^2 
\end{equation}
$$

with the constraints that both $f^\ast_1, \ldots, f^\ast_k$ and $g^\ast_1, \ldots, g^\ast_k$ are collections of orthogonal feature functions; and that the correlation $\sigma_i = \rho(f^\ast_i(\mathsf x), g^\ast_i(\mathsf y))$ are arranged in a descending order. 

We proposed the [H-Score networks](https://gilearning.github.io/HScore/) as a numerical approach, using interconnected neural networks to learn the modal decomposition from data. For two collections of feature functions $f_{[k]}\subset \mathcal {F_X}, g_{[k]} \subset \mathcal {F_Y}$, written in vector form as $\underline{f}, \underline{g}$, the H-score is defined as 

$$
H(\underline{f}, \underline{g}) =\mathrm{trace} (\mathrm{cov} (\underline{f}, \underline{g})) - \frac{1}{2} \mathrm{trace}(\mathbb E[\underline{f} \cdot \underline{f}^T] \cdot \mathbb E[\underline{g} \cdot \underline{g}^T])
$$

|![test image](/assets/Hscorenetwork.png){: width="250" }|![test image](/assets/nested H2.png){: width="450" }|
|<b> H-Score Network </b>|<b> Nested H-Score Network to find features orthogonal to a given $\bar{f}$ </b>|

We showed that maximizing the H-score is equivalent as solving the optimization problem in (1), only without some of the constraints. As a step to enrich our toolbox for selecting feature functions, we also developed the [Nested H-Score networks](https://gilearning.github.io/NestedHScore/) to learn feature functions that are orthogonal to a given functional subspace. We give examples to demonstrate that such a projection operation in the functional space can be quite versatile, including enforcing all the desired constraints in the original modal decomposition problem (1). 

These previous results now include the main tools we need to proceed: to find a limited number of information carrying feature functions and to make projections in functional space, which can all be learned directly from data using interconnected neural networks. In this page, we start to make the case that these are the critical building blocks for more complex multi-variate learning problems. 

## Learning with Side Information



|![test image](/assets/sideinfo.png){: width="350" }|
|<b> H-Score Network </b>|


## Going Forward
We use this example to demonstrate the important fact that multi-variate dependence, represented in the functional space, can be decomposed into a number of subspaces, each corresponding to the dependence of a subset of the variables. Depending on the topology of the network we use to exchange information for inference, we often need to separate the observed information according to these subspaces, and treat different parts of the information differently. And for that purpose, being able to efficiently make projections in the functional space with the nested H-Score networks is critical. 

