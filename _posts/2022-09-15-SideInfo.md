---
title: "Learning with Side Information"
layout: post
---

+ a
+ b
* TOC
{:toc}

> ### The Key Points
>  We introduce our first  multi-variate feature extraction problem, the problem of learning with side information. Here, we wish to select a $k$-dimensional feature of the data $\mathsf x$ that carries useful information to infer the value of the label $\mathsf y$, while excluding the information that a jointly distributed side information $\mathsf s$ can provide. We use this example to develop the underlying geometric structure of multi-variate dependence, and demonstrate how to use the nested H-Score networks to make projections according to these structures to get good solutions. 

## Previously
We started by observing that the dependence between two random variables $\mathsf x$ and $\mathsf y$ can be decomposed into a number of modes, each mode as a simple correlation between a feature $f_i(\mathsf x)$ and a feature $g_i(\mathsf y)$. We formulated the [modal decomposition](https://gilearning.github.io/ModalDecomposition/) as the optimization problem

$$
\begin{equation}
 (f^\ast_{[k]}, g^\ast_{[k]}) =\arg\min_{(f_1, \ldots f_k \in \mathcal {F_X}, g_1, \ldots g_k \in \mathcal {F_Y})} \; \left\Vert \mathrm{PMI}- \left(\sum_{i=1}^k  f_i \otimes g_i\right)\right\Vert^2 
\end{equation}
$$

with the constraints that both $f^\ast_1, \ldots, f^\ast_k$ and $g^\ast_1, \ldots, g^\ast_k$ are collections of orthogonal feature functions; and that the correlation $\sigma_i = \rho(f^\ast_i(\mathsf x), g^\ast_i(\mathsf y))$ are arranged in a descending order. 

We proposed the [H-score networks](https://gilearning.github.io/HScore/) as a numerical approach, using interconnected neural networks to learn the modal decomposition from data. For two collections of feature functions $f_{[k]}\subset \mathcal {F_X}, g_{[k]} \subset \mathcal {F_Y}$, written in vector form as $\underline{f}, \underline{g}$, the H-score is defined as 

$$
H(\underline{f}, \underline{g}) =\mathrm{trace} (\mathrm{cov} (\underline{f}, \underline{g})) - \frac{1}{2} \mathrm{trace}(\mathbb E[\underline{f} \cdot \underline{f}^T] \cdot \mathbb E[\underline{g} \cdot \underline{g}^T])
$$

We showed that maximizing the H-score is equivalent as solving the optimization problem in (1), only without some of the constraints. 

## Learning with Side Information

|![test image](/assets/sideinfo.png){: width="350" }|
|<b> H-Score Network </b>|

## Going Forward
We use this example to demonstrate the important fact that multi-variate dependence, represented in the functional space, can be decomposed into a number of subspaces, each corresponding to the dependence of a subset of the variables. Depending on the topology of the network we use to exchange information for inference, we often need to separate the observed information according to these subspaces, and treat different parts of the information differently. And for that purpose, being able to efficiently make projections in the functional space with the nested H-Score networks is critical. 

