---
title: "Learning with Side Information"
layout: post
---

+ a
+ b
* TOC
{:toc}

> ### The Key Points
>  We introduce our first  multi-variate feature extraction problem, the problem of learning with side information. Here, we wish to select a $k$-dimensional feature of the data $\mathsf x$ that carries useful information to infer the value of the label $\mathsf y$, while excluding the information that a jointly distributed side information $\mathsf s$ can provide. We use this example to develop the underlying geometric structure of multi-variate dependence, and demonstrate how to use the nested H-Score networks to make projections according to these structures to get good solutions. 

## Previously
We started by observing that the dependence between two random variables $\mathsf x$ and $\mathsf y$ can be decomposed into a number of modes, each mode as a simple correlation between a feature $f_i(\mathsf x)$ and a feature $g_i(\mathsf y)$. We formulated the [modal decomposition](https://gilearning.github.io/ModalDecomposition/) as the optimization problem

$$
\begin{equation}
 (f^\ast_{[k]}, g^\ast_{[k]}) =\arg\min_{(f_1, \ldots f_k \in \mathcal {F_X}, g_1, \ldots g_k \in \mathcal {F_Y})} \; \left\Vert \mathrm{PMI}- \left(\sum_{i=1}^k  f_i \otimes g_i\right)\right\Vert^2 
\end{equation}
$$

with the constraints that both $f^\ast_1, \ldots, f^\ast_k$ and $g^\ast_1, \ldots, g^\ast_k$ are collections of orthogonal feature functions; and that the correlation $\sigma_i = \rho(f^\ast_i(\mathsf x), g^\ast_i(\mathsf y))$ are arranged in a descending order. 

We proposed the [H-Score networks](https://gilearning.github.io/HScore/) as a numerical approach, using interconnected neural networks to learn the modal decomposition from data. For two collections of feature functions $f_{[k]}\subset \mathcal {F_X}, g_{[k]} \subset \mathcal {F_Y}$, written in vector form as $\underline{f}, \underline{g}$, the H-score is defined as 

$$
H(\underline{f}, \underline{g}) =\mathrm{trace} (\mathrm{cov} (\underline{f}, \underline{g})) - \frac{1}{2} \mathrm{trace}(\mathbb E[\underline{f} \cdot \underline{f}^T] \cdot \mathbb E[\underline{g} \cdot \underline{g}^T])
$$

|![test image](/assets/Hscorenetwork.png){: width="250" }|![test image](/assets/nested H2.png){: width="450" }|
|<b> H-Score Network </b>|<b> Nested H-Score Network to find features orthogonal to a given $\bar{f}$ </b>|

We showed that maximizing the H-score is equivalent as solving the optimization problem in (1), only without some of the constraints. As a step to enrich our toolbox for selecting feature functions, we also developed the [Nested H-Score networks](https://gilearning.github.io/NestedHScore/) to learn feature functions that are orthogonal to a given functional subspace. We give examples to demonstrate that such a projection operation in the functional space can be quite versatile, including enforcing all the desired constraints in the original modal decomposition problem (1). 

These previous results now include the main tools we need to proceed: to find a limited number of information carrying feature functions and to make projections in functional space, which can all be learned directly from data using interconnected neural networks. In this page, we start to make the case that these are the critical building blocks for more complex multi-variate learning problems. 

## Learning with Side Information

|![test image](/assets/sideinfo.png){: width="350" }|
|<b> Feature Selection with Side Information </b>|

We consider the multi-variate learning problem as shown in the figure. Here, we assume that the data $\mathsf x$, the label $\mathsf y$, and the side information $\mathsf s$ are jointly distributed according to some model $P_{\mathsf {xys}}$ which is not known. Our goal is to find a $k$-dimensional feature function $f_{[k]} = [f_1, \ldots, f_k] : \mathcal X \to \mathbb R^k$, such that when we observe the value of $\mathsf x$, we can infer the value of $\mathsf y$ based on these $k$ features. The only difference between this and a conventional learning problem is that we assume the side information $\mathsf s$ can be observed at the decision maker. That is, our decision is based on the value of $\mathsf s$ and the features: $\widehat{\mathsf y} (f_{[k]}(x), s)$. More importantly, when we select the feature functions, we know that such side information is available at the decision maker. As a start point, we assume we have plenty of samples $(x_i, y_i, s_i), i=1, \ldots$ jointly sampled from the unknown model. 

The main tension of the problem is on having a limited number of $k$ features, and we need to make these features as informative as possible. Intuitively, we want the features to be about the dependence between $\mathsf x$ and $\mathsf y$, so that we can make good predictions; yet we would like to avoid reporting any information that the side information $\mathsf s$ can provide. Related problems can be found in the context of [protecting sensitive information](https://en.wikipedia.org/wiki/Information-theoretic_security), [fairness in machine learning](https://en.wikipedia.org/wiki/Fairness_(machine_learning)), and many other [multi-terminal information theory](http://web.eng.ucsd.edu/~yhk/nit.html) problems. The difficulty here is that we would like learn these feature functions using neural networks, and thus enjoy the computational efficiency and flexibility therein, but we have the additional task to tune the feature functions to avoid overlapping contents. It turns out what we need is a projection operation in the functional space. 


## Decomposition of Multi-Variate Dependence

## Going Forward
We use this example to demonstrate the important fact that multi-variate dependence, represented in the functional space, can be decomposed into a number of subspaces, each corresponding to the dependence of a subset of the variables. Depending on the topology of the network we use to exchange information for inference, we often need to separate the observed information according to these subspaces, and treat different parts of the information differently. And for that purpose, being able to efficiently make projections in the functional space with the nested H-Score networks is critical. 

