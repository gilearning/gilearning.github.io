---
title: "Learning with Side Information"
layout: post
---

+ a
+ b
* TOC
{:toc}

> ### The Key Points
>  We introduce our first  multi-variate feature extraction problem, the problem of learning with side information. Here, we wish to select a $k$-dimensional feature of the data $\mathsf x$ that carries useful information to infer the value of the label $\mathsf y$, while excluding the information that a jointly distributed side information $\mathsf s$ can provide. We use this example to develop the underlying geometric structure of multi-variate dependence, and demonstrate how to use the nested H-Score networks to make projections according to these structures to get good solutions. 

## Previously



## Learning with Side Information

|![test image](/assets/sideinfo.png){: width="350" }|
|<b> H-Score Network </b>|

## Going Forward
We use this example to demonstrate the important fact that multi-variate dependence, represented in the functional space, can be decomposed into a number of subspaces, each corresponding to the dependence of a subset of the variables. Depending on the topology of the network we use to exchange information for inference, we often need to separate the observed information according to these subspaces, and treat different parts of the information differently. And for that purpose, being able to efficiently make projections in the functional space with the nested H-Score networks is critical. 

