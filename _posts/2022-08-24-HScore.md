---
title: "The H Score"
layout: post
---

+ a
+ b
* TOC
{:toc}

> ### The Key Points
> H-score is our first step introducing neural network based methods to compute [modal decomposition](https://gilearning.github.io/ModalDecomposition/) from data. It is a loss metric used in the training of neural networks in a specific configuration. The result is approximately equivalent as a conventional end-to-end neural network with cross entropy loss, with the additional benefit of allowing direct controls of the chosen feature functions. This can be useful in applications with known structures or constraints of feature functions. Conceptually, the use of H-score networks turns the design focus onto functional operations, and serves as a stepping stone for the solutions to more complex problems.



## The H-Score Network
The motivation of [modal decomposition](https://gilearning.github.io/ModalDecomposition/#modal-decomposition) is to find the rank-$k$ approximation 

$$
\begin{equation}
 (f^\ast_1, g^\ast_1), \ldots, (f^\ast_k, g^\ast_k) =\arg\min_{(f_1, \ldots f_k \in \mathcal {F_X}, g_1, \ldots g_k \in \mathcal {F_Y})} \; \left\Vert \mathrm{PMI}- \left(\sum_{i=1}^k  f_i \otimes g_i\right)\right\Vert^2 
\end{equation}
$$


> The above problem is slightly different from our original definition of modal decomposition, where we had a sequential way to find the $k$ modes. This procedure ensures the solution to satisfy a number of constraints: 1) the feature functions all have zero mean and unit variance; 2) the scaling factor of the $i^{th}$ mode, $\sigma_i$, is separated from the normalized features;  3) the feature functions are orthogonal: $\mathbb E[f^\ast_i f^\ast_j] = \mathbb E[g^\ast_i g^\ast_j] = \delta_{ij}$, and 4)there is a descending order of the correlation between $f^\ast_i$ and $g^\ast_i$. Equation (1) is thus different from the modal decomposition. One can either view this optimization to have all the above constraints written in the footnotes and thus be consistent with the modal decomposition, or we could think this in a loose sense as the low rank approximation of the PMI function with no constraints. When we design algorithms to compute the modal decomposition, sometimes only a subset of the above constraints are enforced. We will have a detailed discussion on how these changes affects the solutions later in this page. 

 For convenience we introduce a vector notation: we write column vectors $\underline{f} = [f_1(\mathsf x), \ldots, f_k(\mathsf x)]^T$, $\underline{g} = [g_1(\mathsf y), \ldots, g_k(\mathsf y)]^T$. Now under the local approximation, we replace $\mathrm{PMI}$ by 

$$
\widetilde{\mathrm{PMI}} = \frac{P_{\mathsf{xy}}- P_{\mathsf x} P_{\mathsf y}}{P_{\mathsf x}P_{\mathsf y}}
$$

and compute the norm with respect to $R_{\mathsf {xy}} = P_\mathsf xP_\mathsf y$. This reduces the problem as 

$$
\begin{align*}
(\underline{f}^\ast, \underline{g}^\ast) =&\arg\max_{\underline{f}, \underline{g}} \; \sum_{i=1}^k \mathbb E_{\mathsf {x,y} \sim P_{\mathsf {xy}}}\left[ f_i(\mathsf x) g_i(\mathsf y)\right] -  \mathbb E_{\mathsf x\sim P_\mathsf x}[f_i(\mathsf x)]\cdot \mathbb E_{\mathsf y\sim P_{\mathsf y}}[g_i(\mathsf y)]\\
&\qquad \qquad -\frac{1}{2} \sum_{i=1}^k\sum_{j=1}^k \mathbb E_{\mathsf x\sim P_{\mathsf x}}[f_i(\mathsf x)f_j(\mathsf x)] \cdot \mathbb E_{\mathsf y\sim P_\mathsf y}[g_i(\mathsf y)g_j(\mathsf y)]
\end{align*}
$$

<details>
<summary> with a few steps of algebra hidden in here </summary>

$$
\begin{align*}
\zeta_1^k(P_{\mathsf {xy}}) &= \arg\min \; \left\Vert \widetilde{\mathrm{PMI}}- \left(\sum_{i=1}^k f_i \otimes g_i\right)\right\Vert^2\\
&= \arg \min \; \left\Vert \widetilde{\mathrm{PMI}} \right\Vert^2 - 2 \left\langle \widetilde{\mathrm{PMI}}, \left(\sum_{i=1}^k f_i \otimes g_i\right)\right\rangle + \left\Vert \left(\sum_{i=1}^k  f_i \otimes g_i\right)\right\Vert^2\\
&= \arg\max \; \left\langle \widetilde{\mathrm{PMI}}, \left(\sum_{i=1}^k f_i \otimes g_i\right)\right\rangle -\frac{1}{2} \left\Vert \left(\sum_{i=1}^k f_i \otimes g_i\right)\right\Vert^2\\
&= \arg\max \; \sum_{x,y} P_{\mathsf x}(x) P_{\mathsf y}(y) \cdot \frac{P_{\mathsf {xy}} - P_{\mathsf x}P_{\mathsf y}(y)}{P_{\mathsf x}(x)P_{\mathsf y}(y)}\cdot \left(\sum_{i=1}^k f_i \otimes g_i\right)\\
&\qquad \qquad - \frac{1}{2}\cdot \sum_{xy} P_{\mathsf x}(x) P_{\mathsf y}(y) \cdot \left(\sum_{i=1}^k f_i \otimes g_i\right)^2\\
&= \arg\max \; \sum_{i=1}^k \mathbb E_{\mathsf {x,y} \sim P_{\mathsf {xy}}}\left[ f_i(\mathsf x) g_i(\mathsf y)\right] -  \mathbb E_{\mathsf x\sim P_\mathsf x}[f_i(\mathsf x)]\cdot \mathbb E_{\mathsf y\sim P_{\mathsf y}}[g_i(\mathsf y)]\\
&\qquad \qquad -\frac{1}{2} \sum_{i=1}^k\sum_{j=1}^k \mathbb E_{\mathsf x\sim P_{\mathsf x}}[f_i(\mathsf x)f_j(\mathsf x)] \cdot \mathbb E_{\mathsf y\sim P_\mathsf y}[g_i(\mathsf y)g_j(\mathsf y)]
\end{align*}
$$
</details>

The objective function is what we call the **_H-score_**. 

>**Definition: H-score**
>
>$$
\begin{align*}
H(\underline{f}, \underline{g}) &\stackrel{\Delta}{=} \sum_{i=1}^k \mathrm{cov}[ f_i(\mathsf x) g_i(\mathsf y)] - \frac{1}{2} \sum_{ij} \mathbb E[f_i(\mathsf x)f_j(\mathsf x)] \cdot \mathbb E[g_i(\mathsf y)g_j(\mathsf y)]\\
&=\mathrm{trace} (\mathrm{cov} (\underline{f}, \underline{g})) - \frac{1}{2} \mathrm{trace}(\mathbb E[\underline{f} \cdot \underline{f}^T] \cdot \mathbb E[\underline{g} \cdot \underline{g}^T])
\end{align*}
$$

Now we can imagine a neural network as shown in the figure, where we use two forward neural networks with inputs $\mathsf x$ and $\mathsf y$ separately, each can have its own network architecture of choice, to generate $k$-dimensional features $\underline{f}(\mathsf x)$ and $\underline{g}(\mathsf y)$. Together the two set of features are put together to evaluate the H-score, and we can back propagate the gradients to adjust the network weights to maximize the H-score. We call such a network the **_H-score Network_**. 

|![test image](/assets/autocoder.png){: width="300" }|![test image](/assets/Hscorenetwork.png){: width="250" }|
|<b> Conventional Neural Network </b>|<b> H-Score Network </b>|

**Remarks**

1. In the definition, all expectations are taken with respect to $P_{\mathsf {xy}}$. If the integrand only depends on one of the random variables, then the corresponding marginal distribution $P_{\mathsf x}$ or $P_{\mathsf y}$ can be used instead. 

2. Clearly the hope is to use H-score networks to compute the rank-$k$ approximation of the PMI function and hence the modal decomposition up to $k$ modes. The main difference is that in the definition of modal decomposition, the feature functions are in standard form: zero-mean, unite variance, uncorrelated (orthogonal) to each other, and in descending order of $\sigma_i$'s. In the H-score network, there is no easy way to enforce these constraints. 

3. There are some "hidden force" that helps us to get feature functions in the desired form, which will be discussed in details later. At this point, one can observe that the features that maximizes the H-score must be zero mean. This is because the first term in the definition does not depend on the means $\mathbb E[\underline{f}]$ and $\mathbb E[\underline{g}]$; and the 2nd term is optimized if all features have zero-mean, $\mathbb E[\underline{f} \cdot \underline{f}^T]$ and  $\mathbb E[\underline{g} \cdot \underline{g}^T]$ becomes $\mathrm {cov}[\underline{f}]$ and $\mathrm{cov}[\underline{g}]$, which is in a more "natural" form. 

4. The other aspects of the feature functions are less controllable in H-score networks. We cannot make sure the feature functions are normalized or orthogonal to each other. In fact, if we think of the PMI function as a matrix over $\mathcal {X\times Y}$, and our goal is to approximate this matrix by the product of a $\vert\mathcal X\vert \times k$ matrix of $f$ features and a $k \times \vert\mathcal Y\vert$ matrix of transposed $g$ features, then it is clear that any invertible linear combinations ($k\times k$ matrix multiplied on the right) of the $f$ feature matrix can be canceled by the inverse operations on the $g$ matrix. As a result, there are infinitely many equally good optimizers of the H-score. **The only guarantee one can have is that optimizing the H-score can give a set of $k$ features that span the same subspace as $f^\ast_1, \ldots, f^\ast_k$.** We will address is issue in the next post. 

5. A special case is when $k=1$, i.e., we only try to learn the rank-1 approximation of the model, $\zeta_1 (P_{\mathsf {xy}})$. In this case the learned feature functions are proportional to $f^\ast_1, g^\ast_1$, which is easy to check with the following codes. 
    <details>
    <summary>codes </summary>
    </details> 

6. Computationally, H-score computation is expensive in that the covariance matrices $\mathrm{cov}[\underline{f}]$ and $\mathrm {cov}[\underline{g}]$ both take $O(k^2)$ to evaluate. The good thing is that we don't have to invert these matrices which is common in whitening procedures. In the next section we will focus on the question of "what do we gain by paying this extra cost?"

## A Change in Perspective

As discussed in our post on [modal decomposition](https://gilearning.github.io/ModalDecomposition/), more specifically, in the [definition of modes](https://gilearning.github.io/ModalDecomposition/#a-single-mode), we stated that the goal of low rank approximation of the model, such as in equation (1), is to find an approximation to the true model on a $k$-dimensional exponential family

$$
\hat{P}_{\mathsf{xy}}^{(k)}(x,y) = P_X(x) P_Y(y) \cdot \exp \left( \sum_{i=1}^k \sigma_i \cdot f^\ast_i (x) \cdot g^\ast _i (y)\right) \quad \forall x, y, 
$$

Under the local assumptions, minimizing the L2 error is [a good approximation](https://gilearning.github.io/ModalDecomposition/#divergence-and-fisher-information) to minimizing the K-L divergence
$D(P_{\mathsf {xy}} || \hat{P}_{\mathsf {xy}}^{(k)})$. Put these together, we see that if we replace the true model by the empirical distribution of a batch of samples, the low-rank approximation problem is, with the local approximation, equivalent to the learning over neural networks with [cross-entropy](https://en.wikipedia.org/wiki/Cross_entropy) loss. That is, the standard operations of neural networks and the H-score networks are solving (approximately) the same problem, which is demonstrated with the numerical example above. The question we try to answer here is: what benefit does the H-score network offer? 

This brings us to an _information theoretic_ understanding of neural networks, that the goal of a neural network is to learn a set of  "informative" features. We can now formalize the concept of being informative: the selected features retain as much mutual information between $\mathsf x$ and $\mathsf y$ as possible, or, with the modal decomposition idea in our mind, the features capture a subset of the strongest modes. This view is recently reported in a number of works, such as in this [book](https://book-wright-ma.github.io/), and the studies of [information bottleneck](https://arxiv.org/abs/1503.02406). At a high level, neural networks are viewed as solving lossy compression problems, to keep as much information as possible to predict the labels, while allowing some controlled errors in reconstructing the model.  

The implementation side of this compression/reconstruction view was actually developed earlier than the theory. The idea of [auto-encoders](https://en.wikipedia.org/wiki/Autoencoder)
is to find a representation of the high-dimensional input data $\mathsf x$ in a low-dimensional space, which is called the _representation space_, _feature space_, _semantics space_, etc.,  before reconstructing $\mathsf x$ or some more general target $\mathsf y$ from these features. If the reconstruction is successful, we can be sure that the features retain sufficient "useful" information about the data, while distilling the redundant and irrelevant information, hence we would be comfortable to use these selected features instead of the raw input data for related tasks. 

As shown in the figure, an Autoencoder network consists of an _encoder_ that maps the raw data into features, and a _decoder_ that uses the features to generate a distribution in the reconstruction space, (which we labelled as $\mathsf y$.) This compress/decompress structure closely mirrors that in the [data compression](https://en.wikipedia.org/wiki/Data_compression) problems. 

From an information retrieval point-of-view, such a structure left us with a difficult question: how do we know the reconstruction is good or not? Especially when high dimensional data such as images are involved, classical loss metrics like probability of error or mean-square error do not really reflect the quality of reconstruction well. In practice, there are a number of creative ways to solve this problem, such as employing an extra neural network, [GAN](https://en.wikipedia.org/wiki/Generative_adversarial_network), to learn how to report the quality. 

Conceptually, however, it is clear that such a method of using reconstruction to encourage the encoder to choose information carrying features has its limit. For example, if we would like to have a very aggressive compression, such as to summarize a movie by a couple of words, it is rather unlikely that we could reconstruct from these a few words a sensible version of the entire video sequence, yet we can still define what is a meaningful descriptions of the movie. The point is that the concept of information carrying features is not necessarily associated with reconstruction of the original data, especially in the very deep compression regime. 

This is why we call the H-score network a **_change of perspective_**. All it does is to turn the decoder around, so it becomes another feature selection module that picks out a feature of $\mathsf y$. Now the metric to decide the quality of the selected features are completely defined within the feature space: we do not need to go back to the much higher dimensional space of raw input data when working with these features. 

There is an **additional advantage** of the H-score network: we can directly access the feature functions on both sides. In conventional neural networks, if we know some structure of the input data $\mathsf x$, such as symmetry, regularity, or some constraints that must be satisfied, we often can incorporate that in the design of the network to generate features of $\mathsf x$. CNN for images is a good example. This would be relatively harder if we have knowledge about the labels $\mathsf y$. In H-score networks, $\mathsf x$ and $\mathsf y$ have equal footings. This allows us to implement and learn the processing of $\mathsf y$ exactly the same way as we work on $\mathsf x$, which can be convenient in some cases. 

As an example, we consider a case where $(\mathsf {x,y})$ is a pair of unordered samples, such as two movies that a customer has seen, or two users connected on social network, and we would like to figure out how the two random variables are dependent. Clearly, in this case, both variables are defined on the same alphabet, the joint distribution $P_{\mathsf {xy}}$ is symmetric and the feature functions we should extract satisfy $f^\ast(\cdot) = g^\ast(\cdot)$. To do that, we can use an H-score network where the $f(\cdot)$ and $g(\cdot)$ modules have tied weights and are trained together. As shown in the following demo, this method is directly addressing the problem and hence quite efficient. 

<details>
<summary>code </summary>
</details>

## Going Forward

H-score networks is our first step in processing based on modal decomposition. It offers some moderate benefits and convenience in the standard _bi-variate_ "data-label" problems. To us, it is more of a conceptual step, where our focus of designing neural networks is no longer to predict the labels, but rather shifted to designing the feature functions, since our loss metric is now about the features functions. In a way this is better aligned with our goals, since these features are indeed the carrier of our knowledge, and we would often need to store, exchange, and even use these features for multiple purposes, in the more complex _multi-variate_ problems.

In our next step, we will develop one more tool to directly process the feature functions, which is in a sense to make projections in the functional space, using neural networks. This will be a useful addition to our toolbox before we start to address multi-variate learning problems. 

