---
title: "The H Score"
layout: post
---

+ a
+ b
* TOC
{:toc}

> ### The Key Points
> H-score is our first step introducing neural network based methods to compute [modal decomposition](https://gilearning.github.io/ModalDecomposition/) from data. It is a loss metric used in the training of neural networks in a specific configuration. The result is approximately equivalent as a conventional end-to-end neural network with cross entropy loss, with the additional benefit of allowing direct controls of the chosen feature functions. This can be useful in applications with known structures or constraints of feature functions. Conceptually, the use of H-score networks turns the design focus onto functional operations, and serves as a stepping stone for the solutions to more complex problems.


## Previously

In our previous [post](https://gilearning.github.io/ModalDecomposition/), we made the point that the dependence between two random variables $\mathsf x$ and $\mathsf y$ can be decomposed into a sequence of modes, $\lbrace(\sigma_i, f^\ast_i, g^\ast_i), i=1, 2, \ldots \rbrace$, where $f^\ast_i \in \mathcal {F_X}, g^\ast_i \in \mathcal {F_Y}$ are real-valued functions of $\mathsf x$ and $\mathsf y$, which we call _feature functions_. Each feature function $f^\ast_i$ is correlated with the corresponding $g^\ast_i$ with correlation coefficient $\sigma_i$, and uncorrelated with all other $x$-features $f^\ast_j$ for $j \neq i$, as well as all other $y$-features $g^\ast_j$ for $j\neq i$. The key message is that the statistical dependence between two random variables, which can be quite complex for high dimensional variables, can always be written as a sequence of one-to-one correlated feature pairs, or _modes_. By ordering these modes by their strengths $\sigma_i$, and selecting a truncated subset from the top,  we have a natural way to approximate the model $P_{\mathsf {xy}}$ with a reduced complexity and a graceful loss in the inference performance. 

This decomposition is defined in the L2 sense as 

$$
\lim_{n\to \infty} \left\Vert \mathrm{PMI} - \sum_{i=1}^n \sigma_i \cdot f^\ast_i \otimes g^\ast_i \right\Vert^2 = 0
$$

where $\mathrm{PMI}$ is the [point-wise mutual information](https://en.wikipedia.org/wiki/Pointwise_mutual_information), $\mathrm{PMI}(x,y) = \log \left(\frac{P_{\mathsf {xy}}(x,y)}{P_\mathsf x(x)P_\mathsf y(y)}\right)$ for all $x,y$. The symbol $\otimes$ denotes tensor product, i.e. $f\otimes g$ is a function defined on $\mathcal {X\times Y}$ with $f\otimes g : (x,y) \mapsto f(x)g(y)$. The squared norm $\Vert\cdot \Vert^2$ here is defined on the functional space with respect to a given reference distribution. For example, for $f \in \mathcal {F_X}$, 

$$
\Vert f\Vert^2 = \mathbb E_{\mathsf x \sim R_{\mathsf x}} [ f^2(\mathsf x)]
$$

In this case, the functional space is $\mathcal {F_{X\times Y}}$, and the reference is a product distribution $R_{\mathsf {xy}} = R_\mathsf x R_\mathsf y$, 

The decomposition operations is denoted as $\lbrace \zeta_i \rbrace$, which is a sequence of rank-1 approximation to the approximation error of all previous modes

$$
\zeta_i (P_{\mathsf {xy}}) = (\sigma_i, f^\ast_i, g^\ast_i ) \stackrel{\Delta}{=} \arg\min_{\sigma, f, g}\; \left\Vert \left(\mathrm{PMI} - \sum_{j=1}^{i-1} \sigma_j \cdot f^\ast_j \otimes g^\ast_j \right) - \sigma \cdot f \otimes g\right\Vert^2
$$

where the optimization has the constraint that $f$ and $g$ have zero-mean and unit variance w.r.t. $R_\mathsf x$ and $R_\mathsf y$, resp. 

There are a number of nice properties of the modal decomposition defined above, which are mostly proved under the _local assumptions_. While not always necessary, we will for convenience in our developments assume the model $P_{\mathsf {xy}}$ is in a small neighborhood of reference distribution, which is chosen as the product of the $\mathsf x$ and $\mathsf y$ marginal distributions of the true model, i.e. $R_{\mathsf {xy}} = P_\mathsf x P_\mathsf y$. Based on these assumptions, we have the following facts. 

1. $\langle f^\ast_i, f^\ast_j \rangle = \langle g^\ast_i, g^\ast_j\rangle = \delta_{ij}$ , that is, $\lbrace f^\ast_i\rbrace$ and $\lbrace g^\ast_i \rbrace$ form orthonormal bases respectively on $\mathcal {F_X}$ and $\mathcal {F_Y}$. 
2. $\langle f^\ast_i, g^\ast_j\rangle = \sigma_i \cdot \delta_{ij}$, that is, each feature $f^\ast_i$ is only correlated with the corresponding $g^\ast_i$, with correlation coefficient $\sigma_i$, and uncorrelated with all other $y$-features. 
3. $I(\mathsf x; \mathsf y) \approx \frac{1}{2} \sum_i \sigma_i^2$, that is, the mutual information between $\mathsf x$ and $\mathsf y$, which is a common measure of how much the two random variables are dependent to each other, can be written as the sum of the strengths of individual modes. 

Finally, we also had a numerical example to demonstrate that a small neural network trained with a synthesized dataset using cross entropy loss indeed learned the result of the $\zeta_1$ operation of the true model. This sets up the stage for us to answer the question of this page:

> **What are the good ways to learn the modal decomposition from data?**


## The H-Score Network

Before we start, we will adopt the notation that $a_m^n$ denotes the sequence $\lbrace a_m, a_{m+1}, \ldots, a_n\rbrace $. 

The motivation of [modal decomposition](https://gilearning.github.io/ModalDecomposition/#modal-decomposition) is to find the rank-$k$ approximation 

$$
\begin{equation}
\zeta_1^k(P_\mathsf {xy}) = \arg\min_{(f_i^k \in \mathcal {F_X}, g_i^k \in \mathcal {F_Y})} \; \left\Vert \mathrm{PMI}- \left(\sum_{i=1}^k  f_i \otimes g_i\right)\right\Vert^2 
\end{equation}
$$

Here, we do not assume that the feature functions $f_1^k, g_1^k$ to have zero-mean and unit variance, since in computing these feature functions we do not always have a mechanism to enforce such constraints. Now under the local approximation, we replace $\mathrm{PMI}$ by 

$$
\widetilde{\mathrm{PMI}} = \frac{P_{\mathsf{xy}}- P_{\mathsf x} P_{\mathsf y}}{P_{\mathsf x}P_{\mathsf y}}
$$

and compute the norm with respect to $R_{\mathsf {xy}} = P_\mathsf xP_\mathsf y$. This reduces the problem as 

$$
\begin{align*}
\zeta_1^k(P_{\mathsf {xy}})&=\arg\max_{f_1^k, g_1^k} \; \sum_{i=1}^k \mathbb E_{\mathsf {x,y} \sim P_{\mathsf {xy}}}\left[ f_i(\mathsf x) g_i(\mathsf y)\right] -  \mathbb E_{\mathsf x\sim P_\mathsf x}[f_i(\mathsf x)]\cdot \mathbb E_{\mathsf y\sim P_{\mathsf y}}[g_i(\mathsf y)]\\
&\qquad \qquad -\frac{1}{2} \sum_{i=1}^k\sum_{j=1}^k \mathbb E_{\mathsf x\sim P_{\mathsf x}}[f_i(\mathsf x)f_j(\mathsf x)] \cdot \mathbb E_{\mathsf y\sim P_\mathsf y}[g_i(\mathsf y)g_j(\mathsf y)]
\end{align*}
$$

<details>
<summary> with a few steps of algebra hidden in here </summary>

$$
\begin{align*}
\zeta_1^k(P_{\mathsf {xy}}) &= \arg\min \; \left\Vert \widetilde{\mathrm{PMI}}- \left(\sum_{i=1}^k f_i \otimes g_i\right)\right\Vert^2\\
&= \arg \min \; \left\Vert \widetilde{\mathrm{PMI}} \right\Vert^2 - 2 \left\langle \widetilde{\mathrm{PMI}}, \left(\sum_{i=1}^k f_i \otimes g_i\right)\right\rangle + \left\Vert \left(\sum_{i=1}^k  f_i \otimes g_i\right)\right\Vert^2\\
&= \arg\max \; \left\langle \widetilde{\mathrm{PMI}}, \left(\sum_{i=1}^k f_i \otimes g_i\right)\right\rangle -\frac{1}{2} \left\Vert \left(\sum_{i=1}^k f_i \otimes g_i\right)\right\Vert^2\\
&= \arg\max \; \sum_{x,y} P_{\mathsf x}(x) P_{\mathsf y}(y) \cdot \frac{P_{\mathsf {xy}} - P_{\mathsf x}P_{\mathsf y}(y)}{P_{\mathsf x}(x)P_{\mathsf y}(y)}\cdot \left(\sum_{i=1}^k f_i \otimes g_i\right)\\
&\qquad \qquad - \frac{1}{2}\cdot \sum_{xy} P_{\mathsf x}(x) P_{\mathsf y}(y) \cdot \left(\sum_{i=1}^k f_i \otimes g_i\right)^2\\
&= \arg\max \; \sum_{i=1}^k \mathbb E_{\mathsf {x,y} \sim P_{\mathsf {xy}}}\left[ f_i(\mathsf x) g_i(\mathsf y)\right] -  \mathbb E_{\mathsf x\sim P_\mathsf x}[f_i(\mathsf x)]\cdot \mathbb E_{\mathsf y\sim P_{\mathsf y}}[g_i(\mathsf y)]\\
&\qquad \qquad -\frac{1}{2} \sum_{i=1}^k\sum_{j=1}^k \mathbb E_{\mathsf x\sim P_{\mathsf x}}[f_i(\mathsf x)f_j(\mathsf x)] \cdot \mathbb E_{\mathsf y\sim P_\mathsf y}[g_i(\mathsf y)g_j(\mathsf y)]
\end{align*}
$$
</details>

The objective function is what we call the **_H-score_**. For convenience we would like to introduce a vector notation: we define column vectors $\underline{f} = [f_1(\mathsf x), \ldots, f_k(\mathsf x)]^T$, $\underline{g} = [g_1(\mathsf y), \ldots, g_k(\mathsf y)]^T$. 

>**Definition: H-score**
>
>$$
\begin{align*}
H(\underline{f}, \underline{g}) &\stackrel{\Delta}{=} \sum_{i=1}^k \mathrm{cov}[ f_i(\mathsf x) g_i(\mathsf y)] - \frac{1}{2} \sum_{ij} \mathbb E[f_i(\mathsf x)f_j(\mathsf x)] \cdot \mathbb E[g_i(\mathsf y)g_j(\mathsf y)]\\
&=\mathrm{trace} (\mathrm{cov} (\underline{f}, \underline{g})) - \frac{1}{2} \mathrm{trace}(\mathbb E[\underline{f} \cdot \underline{f}^T] \cdot \mathbb E[\underline{g} \cdot \underline{g}^T])
\end{align*}
$$

Now we can imagine a neural network as shown in the figure, where we use two forward neural networks with inputs $\mathsf x$ and $\mathsf y$ separately, each can have its own network architecture of choice, to generate $k$-dimensional features $\underline{f}(\mathsf x)$ and $\underline{g}(\mathsf y)$. Together the two set of features are put together to evaluate the H-score, and we can back propagate the gradients to adjust the network weights to maximize the H-score. We call such a network the **_H-score Network_**. 

|![test image](/assets/Hscorenetwork.png){: width="250" }|![test image](/assets/autocoder.png){: width="300" }|
|<b> H-Score Network </b>|<b> Autocoder </b>|

**Remarks**

1. In the definition, all expectations are taken with respect to $P_{\mathsf {xy}}$. If the integrand only depends on one of the random variables, then the corresponding marginal distribution $P_{\mathsf x}$ or $P_{\mathsf y}$ can be used instead. 

2. Clearly the hope is to use H-score networks to compute the rank-$k$ approximation of the PMI function and hence the modal decomposition up to $k$ modes. The main difference is that in the definition of modal decomposition, the feature functions are in standard form: zero-mean, unite variance, uncorrelated (orthogonal) to each other, and in descending order of $\sigma_i$'s. In the H-score network, there is no easy way to enforce these constraints. 

3. There are some "hidden force" that helps us to get feature functions in the desired form, which will be discussed in details later. At this point, one can observe that the features that maximizes the H-score must be zero mean. This is because the first term in the definition does not depend on the means $\mathbb E[\underline{f}]$ and $\mathbb E[\underline{g}]$; and the 2nd term is optimized if all features have zero-mean, $\mathbb E[\underline{f} \cdot \underline{f}^T]$ and  $\mathbb E[\underline{g} \cdot \underline{g}^T]$ becomes $\mathrm {cov}[\underline{f}]$ and $\mathrm{cov}[\underline{g}]$, which is in a more "natural" form. 

4. The other aspects of the feature functions are less controllable in H-score networks. We cannot make sure the feature functions are normalized or orthogonal to each other. In fact, if we think of the PMI function as a matrix over $\mathcal {X\times Y}$, and our goal is to approximate this matrix by the product of a $\vert\mathcal X\vert \times k$ matrix of $f$ features and a $k \times \vert\mathcal Y\vert$ matrix of transposed $g$ features, then it is clear that any invertible linear combinations ($k\times k$ matrix multiplied on the right) of the $f$ feature matrix can be canceled by the inverse operations on the $g$ matrix. As a result, there are infinitely many equally good optimizers of the H-score. **The only guarantee one can have is that optimizing the H-score can give a set of $k$ features that span the same subspace as $f^\ast_1, \ldots, f^\ast_k$.** We will address is issue in the next post. 

5. A special case is when $k=1$, i.e., we only try to learn the rank-1 approximation of the model, $\zeta_1 (P_{\mathsf {xy}})$. In this case the learned feature functions are proportional to $f^\ast_1, g^\ast_1$, which is easy to check with the following codes. 
    <details>
    <summary>codes </summary>
    </details> 

6. Computationally, H-score computation is expensive in that the covariance matrices $\mathrm{cov}[\underline{f}]$ and $\mathrm {cov}[\underline{g}]$ both take $O(k^2)$ to evaluate. The good thing is that we don't have to invert these matrices which is common in whitening procedures. In the next section we will focus on the question of "what do we gain by paying this extra cost?"

## A Change in Perspective

As discussed in our post on [modal decomposition](https://gilearning.github.io/ModalDecomposition/), more specifically, in the [definition of modes](https://gilearning.github.io/ModalDecomposition/#a-single-mode), we stated that the goal of low rank approximation of the model, such as in equation (1), is to find an approximation to the true model on a $k$-dimensional exponential family

$$
\hat{P}_{\mathsf{xy}}^{(k)}(x,y) = P_X(x) P_Y(y) \cdot \exp \left( \sum_{i=1}^k \sigma_i \cdot f^\ast_i (x) \cdot g^\ast _i (y)\right) \quad \forall x, y, 
$$

Under the local assumptions, minimizing the L2 error is [a good approximation](https://gilearning.github.io/ModalDecomposition/#divergence-and-fisher-information) to minimizing the K-L divergence
$D(P_{\mathsf {xy}} || \hat{P}_{\mathsf {xy}}^{(k)})$. Put these together, we see that if we replace the true model by the empirical distribution of a batch of samples, the low-rank approximation problem is, with the local approximation, equivalent to the learning over neural networks with [cross-entropy](https://en.wikipedia.org/wiki/Cross_entropy) loss. That is, the standard operations of neural networks and the H-score networks are solving (approximately) the same problem, which is demonstrated with the numerical example above. The question we try to answer here is: what benefit does the H-score network offer? 

This brings us to an _information theoretic_ understanding of neural networks, that the goal of a neural network is to learn a set of  "informative" features. We can now formalize the concept of being informative: the selected features retain as much mutual information between $\mathsf x$ and $\mathsf y$ as possible, or, with the modal decomposition idea in our mind, the features capture a subset of the strongest modes. This view is recently reported in a number of works, such as in this [book](https://book-wright-ma.github.io/), and the studies of [information bottleneck](https://arxiv.org/abs/1503.02406). At a high level, neural networks are viewed as solving lossy compression problems, to keep as much information as possible to predict the labels, while allowing some of errors in reconstructing the model.  

The implementation side of this compression/reconstruction view was actually developed earlier than the theory. The idea of [autocoders](https://en.wikipedia.org/wiki/Autoencoder)
is to find a representation of the high-dimensional input data $\mathsf x$ in a low-dimensional space, which is called the _representation space_, _feature space_, _semantics space_, etc.,  before reconstructing $\mathsf x$ or some more general target $\mathsf y$ from these features. If the reconstruction is successful, we can be sure that the features retain sufficient "useful" information about the data, while distilling the redundant and irrelevant information, hence we would be comfortable to use these selected features instead of the raw input data for related tasks. 

As shown in the figure, an Autoencoder network consists of an _encoder_ that maps the raw data into features, and a _decoder_ that uses the features to generate a distribution in the reconstruction space, (which we labelled as $\mathsf y$.) This compress/decompress structure closely mirrors that in the [data compression](https://en.wikipedia.org/wiki/Data_compression) problems. 

From an information retrieval point-of-view, such a structure left us with a difficult question: how do we know the reconstruction is good or not? Especially when high dimensional data such as images are involved, classical loss metrics like probability of error or mean-square error do not really reflect the quality of reconstruction well. In practice, there are a number of creative ways to solve this problem, such as employing an extra neural network, [GAN](https://en.wikipedia.org/wiki/Generative_adversarial_network), to learn how to report the quality. 

Conceptually, however, it is clear that such a method of using reconstruction to encourage the encoder to choose information carrying features has its limit. For example, if we would like to have a very aggressive compression, such as to summarize a movie by a a couple of words, it is rather unlikely that we could reconstruct from these a few words a sensible version of the entire video sequence, yet we can still define what is a meaningful descriptions of the movie. The point is that the concept of information carrying features is not necessarily associated with reconstruction of the original data, especially in the very deep compression regime. 

This is why we call the H-score network a **_change of perspective_**. All it does is to turn the decoder around, so it becomes another feature selection module that picks out a feature of $\mathsf y$. Now the metric to decide how good are the selected features are completely defined within the feature space: we do not need to go back to the much higher dimensional data space when working with these features. 

## Going Forward
