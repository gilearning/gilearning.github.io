---
title: "The H Score"
layout: post
---

+ a
+ b
* TOC
{:toc}

> ### The Key Points
> H-score is our first step introducing neural network based methods to compute [modal decomposition](https://gilearning.github.io/ModalDecomposition/) from data. It is a loss metric used in the training of neural networks in a specific configuration. The result is approximately equivalent as a conventional end-to-end neural network with cross entropy loss, with the additional benefit of allowing direct controls of the chosen feature functions. This can be useful in applications with known structures or constraints of feature functions. Conceptually, the use of H-score networks turns the design focus onto functional operations, and serves as a stepping stone for the solutions to more complex problems.


## Previously

In our previous [post](https://gilearning.github.io/ModalDecomposition/), we made the point that the dependence between two random variables $\mathsf x$ and $\mathsf y$ can be decomposed into a sequence of modes, $\lbrace(\sigma_i, f^\ast_i, g^\ast_i), i=1, 2, \ldots \rbrace$, where $f^\ast_i \in \mathcal {F_X}, g^\ast_i \in \mathcal {F_Y}$ are real-valued functions of $\mathsf x$ and $\mathsf y$, which we call _feature functions_. Each feature function $f^\ast_i$ is correlated with the corresponding $g^\ast_i$ with correlation coefficient $\sigma_i$, and uncorrelated with all other $x$-features $f^\ast_j$ for $j \neq i$, as well as all other $y$-features $g^\ast_j$ for $j\neq i$. The key message is that the statistical dependence between two random variables, which can be quite complex for high dimensional variables, can always be written as a sequence of one-to-one correlated feature pairs, or _modes_. By ordering these modes by their strengths $\sigma_i$, and selecting a truncated subset from the top,  we have a natural way to approximate the model $P_{\mathsf {xy}}$ with a reduced complexity and a graceful loss in the inference performance. 

This decomposition is defined in the L2 sense as 

$$
\lim_{n\to \infty} \left\Vert \mathrm{PMI} - \sum_{i=1}^n \sigma_i \cdot f^\ast_i \otimes g^\ast_i \right\Vert^2 = 0
$$

where $\mathrm{PMI}$ is the [point-wise mutual information](https://en.wikipedia.org/wiki/Pointwise_mutual_information), $\mathrm{PMI}(x,y) = \log \left(\frac{P_{\mathsf {xy}}(x,y)}{P_\mathsf x(x)P_\mathsf y(y)}\right)$ for all $x,y$. The symbol $\otimes$ denotes tensor product, i.e. $f\otimes g$ is a function defined on $\mathcal {X\times Y}$ with $f\otimes g : (x,y) \mapsto f(x)g(y)$. The squared norm $\Vert\cdot \Vert^2$ here is defined on the functional space with respect to a given reference distribution. For example, for $f \in \mathcal {F_X}$, 

$$
\Vert f\Vert^2 = \mathbb E_{\mathsf x \sim R_{\mathsf x}} [ f^2(\mathsf x)]
$$

In this case, the functional space is $\mathcal {F_{X\times Y}}$, and the reference is a product distribution $R_{\mathsf {xy}} = R_\mathsf x R_\mathsf y$, 

The decomposition operations is denoted as $\lbrace \zeta_i \rbrace$, which is a sequence of rank-1 approximation to the approximation error of all previous modes

$$
\zeta_i (P_{\mathsf {xy}}) = (\sigma_i, f^\ast_i, g^\ast_i ) \stackrel{\Delta}{=} \arg\min_{\sigma, f, g}\; \left\Vert \left(\mathrm{PMI} - \sum_{j=1}^{i-1} \sigma_j \cdot f^\ast_j \otimes g^\ast_j \right) - \sigma \cdot f \otimes g\right\Vert^2
$$

where the optimization has the constraint that $f$ and $g$ have zero-mean and unit variance w.r.t. $R_\mathsf x$ and $R_\mathsf y$, resp. 

There are a number of nice properties of the modal decomposition defined above, which are mostly proved under the _local assumptions_. While not always necessary, we will for convenience in our developments assume the model $P_{\mathsf {xy}}$ is in a small neighborhood of reference distribution, which is chosen as the product of the $\mathsf x$ and $\mathsf y$ marginal distributions of the true model, i.e. $R_{\mathsf {xy}} = P_\mathsf x P_\mathsf y$. Based on these assumptions, we have the following facts. 

1. $\langle f^\ast_i, f^\ast_j \rangle = \langle g^\ast_i, g^\ast_j\rangle = \delta_{ij}$ , that is, $\lbrace f^\ast_i\rbrace$ and $\lbrace g^\ast_i \rbrace$ form orthonormal bases respectively on $\mathcal {F_X}$ and $\mathcal {F_Y}$. 
2. $\langle f^\ast_i, g^\ast_j\rangle = \sigma_i \cdot \delta_{ij}$, that is, each feature $f^\ast_i$ is only correlated with the corresponding $g^\ast_i$, with correlation coefficient $\sigma_i$, and uncorrelated with all other $y$-features. 
3. $I(\mathsf x; \mathsf y) \approx \frac{1}{2} \sum_i \sigma_i^2$, that is, the mutual information between $\mathsf x$ and $\mathsf y$, which is a common measure of how much the two random variables are dependent to each other, can be written as the sum of the strengths of individual modes. 

Finally, we also had a numerical example to demonstrate that a small neural network trained with a synthesized dataset using cross entropy loss indeed learned the result of the $\zeta_1$ operation of the true model. This sets up the stage for us to answer the question of this page:

> **What are the good ways to learn the modal decomposition from data?**


## H-Score Network

## A Change in Perspective

## Going Forward
